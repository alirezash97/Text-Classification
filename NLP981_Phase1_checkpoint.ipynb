{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP981_Phase1-checkpoint.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alirezash97/Text-Classification/blob/master/NLP981_Phase1_checkpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Tduc6QDQz1H"
      },
      "source": [
        "# NLP981 Final Project - Phase #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P0F0hIUs7oCS"
      },
      "source": [
        "*   Instructor: Javad PourMostafa\n",
        "*   Teaching Assistant: Parsa Abbasi\n",
        "*   University of Guilan, 1st semester of 2019\n",
        "*   GitHub repository : *https://github.com/JoyeBright/NLP*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rVC1mwiaOZMI",
        "outputId": "008998c2-5ede-48b7-9b2b-bb7bc3256c3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "!pip install nltk\n",
        "!pip install stopwords_guilannlp\n",
        "!pip install hazm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: stopwords_guilannlp in /usr/local/lib/python3.6/dist-packages (13.2019.3.5)\n",
            "Requirement already satisfied: hazm in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.6/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: libwapiti>=0.2.1; platform_system != \"Windows\" in /usr/local/lib/python3.6/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.3->hazm) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aUO9K1mKO2EB",
        "outputId": "30d14edd-1352-407b-8167-eb14f03a9069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import nltk\n",
        "from tqdm import tqdm\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hJiCXKiUNsUQ",
        "outputId": "1ee59bdc-4bd9-4c7c-a099-d74c86e9fabf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentence = \"Hello, world!\"\n",
        "print (nltk.word_tokenize(sentence))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_aE9xQhhQ6XS"
      },
      "source": [
        "It's the first phase of your final project for the *NLP981* course. The main idea behind this phase is to portray the develope side of *NLP*.\n",
        "\n",
        "You must code inside of this python notebook. I highly recommend you to use the *Google Colab* environment. \n",
        "\n",
        "If you have any questions, feel free to ask.\n",
        "You can use [*Quera*](https://quera.ir/course/4385/) platform for your general questions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EgS3aCY358dV"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6FG4cndz6DDq"
      },
      "source": [
        "A category predictor is going to build at this phase of the project.\n",
        "\n",
        "The predictor gets a text as input and predicts a category for that.\n",
        "\n",
        "For this purpose, you need to :\n",
        "\n",
        "1.   Load the dataset\n",
        "2.   Preprocess the text data\n",
        "3.   Implement a word representation method to represent each text as a numeric vector\n",
        "4.   Implement a classification model and train that using the training set\n",
        "5.   Predict a category for each of validation data using implemented model\n",
        "6.   Measure your work using confusion matrix and some common metrics\n",
        "\n",
        "**Important Note:** You can use any library you want in sections 1 and 2. But everything in section 3-6 need to be coded purely.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0acil66KQ8A_"
      },
      "source": [
        "## 1) Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "haH1kfj3Q9tf"
      },
      "source": [
        "The dataset you will use in this phase is called *Divar* that released by the *CafeBazaar* research team.\n",
        "\n",
        "It contains more than 900,000 posts of the *Divar* ads platform. We split this dataset into training, validation, and testing sets.\n",
        "\n",
        "The testing set is not accessible for you, and we use them to evaluate your work on the presentation day.\n",
        "\n",
        "You can download the dataset files (training and validation sets) directly from the following link :\n",
        "\n",
        "> *https://drive.google.com/open?id=1oj-fqpymjDr8QsOK-zQliiqXbVqakrFo*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UdIRg1UBSOEi"
      },
      "source": [
        "### 1.1) Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gwCPJjaSQukm",
        "colab": {}
      },
      "source": [
        "# Import the training and validation sets here\n",
        "import stopwords_guilannlp as sw\n",
        "from google.colab import files\n",
        "from itertools import chain\n",
        "import plotly as py\n",
        "import plotly.graph_objs as go\n",
        "import time\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import regex as re\n",
        "import sys\n",
        "from hazm import *\n",
        "import pandas as pd\n",
        "trainset = pd.read_csv('drive/My Drive/NLP_dataset/trainset.csv')\n",
        "validationset = pd.read_csv('drive/My Drive/NLP_dataset/validationset.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Pt_-4u5VaE4",
        "colab_type": "code",
        "outputId": "2a17fb02-5a25-4ac9-c537-fd9887e3748e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MWHwVPkfS-OX"
      },
      "source": [
        "### 1.2) Analyzing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KlIKCffqU4AM"
      },
      "source": [
        "Display the top 10 rows of the train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gw0EuzrFVIat",
        "outputId": "a18f5932-74e8-4467-bf4b-9e7c04b72efe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        }
      },
      "source": [
        "# Your code\n",
        "trainset.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>archive_by_user</th>\n",
              "      <th>brand</th>\n",
              "      <th>cat1</th>\n",
              "      <th>cat2</th>\n",
              "      <th>cat3</th>\n",
              "      <th>city</th>\n",
              "      <th>created_at</th>\n",
              "      <th>desc</th>\n",
              "      <th>id</th>\n",
              "      <th>image_count</th>\n",
              "      <th>mileage</th>\n",
              "      <th>platform</th>\n",
              "      <th>price</th>\n",
              "      <th>title</th>\n",
              "      <th>type</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>282086</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>personal</td>\n",
              "      <td>clothing-and-shoes</td>\n",
              "      <td>shoes-belt-bag</td>\n",
              "      <td>Tehran</td>\n",
              "      <td>Monday 11AM</td>\n",
              "      <td>چکمه یکبار پوشیده شده قیمت 42\\nکفش قهوه ای سوخ...</td>\n",
              "      <td>12875614029625</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>mobile</td>\n",
              "      <td>42000</td>\n",
              "      <td>سایز 40</td>\n",
              "      <td>women</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>762753</td>\n",
              "      <td>True</td>\n",
              "      <td>Samsung::سامسونگ</td>\n",
              "      <td>electronic-devices</td>\n",
              "      <td>mobile-tablet</td>\n",
              "      <td>mobile-phones</td>\n",
              "      <td>Tehran</td>\n",
              "      <td>Wednesday 12PM</td>\n",
              "      <td>گوشی رو تا حالا باز نکردم و تو جعبه پلمپه از د...</td>\n",
              "      <td>16051997226596</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>mobile</td>\n",
              "      <td>850000</td>\n",
              "      <td>گوشی سامسونگ a3 2016</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>805240</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>personal</td>\n",
              "      <td>jewelry-and-watches</td>\n",
              "      <td>watches</td>\n",
              "      <td>Tehran</td>\n",
              "      <td>Tuesday 09AM</td>\n",
              "      <td>ساعت هیچ مشکلی ندارد اصل اصل هستش چون دیگه دست...</td>\n",
              "      <td>51715717387979</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>mobile</td>\n",
              "      <td>130000</td>\n",
              "      <td>ساعت زنانه اسپریت اصل</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>556730</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>personal</td>\n",
              "      <td>baby-and-toys</td>\n",
              "      <td>personal-toys</td>\n",
              "      <td>Tehran</td>\n",
              "      <td>Thursday 06PM</td>\n",
              "      <td>دوچرخه از هرلحاظ سالمه و فقط مدت کوتاهی استفاد...</td>\n",
              "      <td>9381204619687</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>mobile</td>\n",
              "      <td>200000</td>\n",
              "      <td>فروش یک عدد دوچرخه مارک پرادو بسیار سالم</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>727332</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>leisure-hobbies</td>\n",
              "      <td>hobby-collectibles</td>\n",
              "      <td>coin-stamp</td>\n",
              "      <td>Tehran</td>\n",
              "      <td>Wednesday 09PM</td>\n",
              "      <td>14 اسکناس مطابق تصویر همه باهم 200 هزار تومان\\...</td>\n",
              "      <td>47687757949429</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>web</td>\n",
              "      <td>-1</td>\n",
              "      <td>14 اسکناس کلکسیونی code1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>805039</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>leisure-hobbies</td>\n",
              "      <td>sport-leisure</td>\n",
              "      <td>ball-sports</td>\n",
              "      <td>Tehran</td>\n",
              "      <td>Tuesday 11AM</td>\n",
              "      <td>فوتبال دستی حرفه ای تاشو درحد نو فقط 2ماه استف...</td>\n",
              "      <td>35888748102799</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>mobile</td>\n",
              "      <td>480000</td>\n",
              "      <td>فوتبال دستی حرفه ای</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>812617</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>for-the-home</td>\n",
              "      <td>furniture-and-home-decore</td>\n",
              "      <td>lighting</td>\n",
              "      <td>Tehran</td>\n",
              "      <td>Thursday 08AM</td>\n",
              "      <td>لوستر نو برای اتاق کودک</td>\n",
              "      <td>61553946516880</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>mobile</td>\n",
              "      <td>25000</td>\n",
              "      <td>لوستر</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>295730</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>vehicles</td>\n",
              "      <td>parts-accessories</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Isfahan</td>\n",
              "      <td>Monday 10PM</td>\n",
              "      <td>4 عدد رینگ اهنی ال 90 سالم قیمت از شما من فروش...</td>\n",
              "      <td>21090520055338</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>mobile</td>\n",
              "      <td>-1</td>\n",
              "      <td>رینگ اهنی ال 90</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>777605</td>\n",
              "      <td>False</td>\n",
              "      <td>نیسان::Nissan</td>\n",
              "      <td>vehicles</td>\n",
              "      <td>cars</td>\n",
              "      <td>light</td>\n",
              "      <td>Shiraz</td>\n",
              "      <td>Wednesday 01PM</td>\n",
              "      <td>بیمه تا۹۵/۹/۲۷,کف دستی رنگ درعقب</td>\n",
              "      <td>27046580911179</td>\n",
              "      <td>2</td>\n",
              "      <td>380000.0</td>\n",
              "      <td>mobile</td>\n",
              "      <td>40000000</td>\n",
              "      <td>نیسان سرانزا مدل۸۳</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>797079</td>\n",
              "      <td>False</td>\n",
              "      <td>پراید صندوق‌دار::Pride</td>\n",
              "      <td>vehicles</td>\n",
              "      <td>cars</td>\n",
              "      <td>light</td>\n",
              "      <td>Tehran</td>\n",
              "      <td>Tuesday 03PM</td>\n",
              "      <td>عقب وجلوپلمب بیرنگ لاستیک نو به شرط مصرف کننده...</td>\n",
              "      <td>42106059496341</td>\n",
              "      <td>0</td>\n",
              "      <td>109000.0</td>\n",
              "      <td>mobile</td>\n",
              "      <td>14000000</td>\n",
              "      <td>پراید سفید</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1390</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  ...   type  year\n",
              "0           0        282086  ...  women   NaN\n",
              "1           1        762753  ...    NaN   NaN\n",
              "2           2        805240  ...    NaN   NaN\n",
              "3           3        556730  ...    NaN   NaN\n",
              "4           4        727332  ...    NaN   NaN\n",
              "5           5        805039  ...    NaN   NaN\n",
              "6           6        812617  ...    NaN   NaN\n",
              "7           7        295730  ...    NaN   NaN\n",
              "8           8        777605  ...    NaN  1383\n",
              "9           9        797079  ...    NaN  1390\n",
              "\n",
              "[10 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iqfkJST6VMXt"
      },
      "source": [
        "How many data (rows) stored in the training and validation sets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jkiaz92zVVpy",
        "colab": {}
      },
      "source": [
        "train_len = trainset.shape[0]\n",
        "valid_len = validationset.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9aw0x_s3_b4U"
      },
      "source": [
        "How many posts are in each category (First level categories)? (Based on training set)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "31PFgy46_ntw",
        "outputId": "0bb51553-73cf-4ca1-a05b-fc173dc67839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# Your code\n",
        "print(trainset.groupby('cat1').size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat1\n",
            "businesses             45660\n",
            "electronic-devices    122905\n",
            "for-the-home          214955\n",
            "leisure-hobbies        61676\n",
            "personal              102804\n",
            "vehicles              152000\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KUejNXljlZD1"
      },
      "source": [
        "## 2) Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-j1EilpbAi-W"
      },
      "source": [
        "There are two kinds of text data in the dataset: *Title* and *Description*.\n",
        "You can use one or both of them as text inputs of your classification model. Choose a composition that gives you a higher measuring score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cLT50bemjw3K"
      },
      "source": [
        "You need to apply some preprocessing procedures on your text data first. We want at least **4** preprocessing step from you. It can be removing stop words, removing punctation, removing or replacing digits, stemming, lemmatizing, normalization, and so on.\n",
        "\n",
        "You can use the [*Stopwords Guilan NLP*](https://github.com/JoyeBright/stopwords_guilannlp) library to access a collection of Persian stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xPQYAeEnlrcc",
        "colab": {}
      },
      "source": [
        "def preprocessing(document):\n",
        "  # Your preprocessing steps\n",
        "    \n",
        "    cleared_text = []\n",
        "    normalizer = Normalizer()\n",
        "    lemmatizer = Lemmatizer()\n",
        "    stemmer = Stemmer()\n",
        "    \n",
        "    \n",
        "    \n",
        "    for text in tqdm(document) :\n",
        "        \n",
        "        lemmatized = []\n",
        "        filtered_tokens = []\n",
        "        \n",
        "    # step 1 \n",
        "        \n",
        "        normalized_text = normalizer.normalize(text)\n",
        "    \n",
        "    # step 2\n",
        "    \n",
        "        tokens = nltk.word_tokenize(normalized_text)\n",
        "        \n",
        "    \n",
        "    # step 3\n",
        "    \n",
        "        stopwords = sw.stopwords_output(\"Persian\", \"nar\")\n",
        "        for w in tokens: \n",
        "            if w not in stopwords :\n",
        "                filtered_tokens.append(w)\n",
        "                \n",
        "    # step 4\n",
        "        filtered_tokens = [token for token in filtered_tokens if not token.isdigit()]\n",
        "   \n",
        "    # step 5\n",
        "        \n",
        "        for i in range (0, len(filtered_tokens)):\n",
        "            if len(lemmatizer.lemmatize(filtered_tokens[i])) >= 1:\n",
        "                lemmatized.append(lemmatizer.lemmatize(filtered_tokens[i]))\n",
        "            else :\n",
        "                pass\n",
        "    \n",
        "        \n",
        "        cleared_text.append(lemmatized)  \n",
        "\n",
        "\n",
        "    return cleared_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "izN13hPulonI"
      },
      "source": [
        "## 3) Word Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UeC-rd4DMSUb"
      },
      "source": [
        "As you know, classification models can't deal with strings directly, and you have to represent your texts in a numerical form."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S1rGMFJmqKwm"
      },
      "source": [
        "### 3.1) Tf-idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VfPOmAF6qOF0"
      },
      "source": [
        "You have to implement the tf-idf vectorization method from scratch in this step. \n",
        "\n",
        "Furthermore, a function must be implemented that gives a text input and return a tf-idf vectorized representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nTZCMdqft4Ic"
      },
      "source": [
        "$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)$$\n",
        "\n",
        "*tf* (term-frequency) is the count of occurrences of the word `t` in specific text `d`.\n",
        "\n",
        "*idf* (inverse document-frequency) is term that is inversely proportional to the number of texts with the given word. It can be calculated this way:\n",
        "$$\\text{idf}(t) = \\text{log}\\frac{1 + n_d}{1 + n_{d(t)}} + 1$$\n",
        "where $n_d$ is the whole number of texts and $n_{d(t)}$ is the number of texts with the word `t`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FA9EpAu3RVT",
        "colab_type": "code",
        "outputId": "6c405278-cbed-4c34-b26e-4987299e20bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##########################  train set for learning #######################\n",
        "start_point = 0   # form #row \n",
        "end_point = 2000   # to   #row\n",
        "################################################################################\n",
        "H = trainset.iloc[start_point : end_point,9].values\n",
        "document = preprocessing(H)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:06<00:00, 327.87it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZVwNZTGLqm20",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "def tf(document):\n",
        "    \n",
        "    tf = []\n",
        "    \n",
        "    for text in tqdm(document):\n",
        "        tf_text = {}\n",
        "        for word in text:\n",
        "            tf_text[word] = 0\n",
        "        for word in text:\n",
        "            tf_text[word] += 1\n",
        "        for word in tf_text:\n",
        "            # print(len(text))\n",
        "            tf_text[word] = tf_text[word] / len(text)\n",
        "        tf.append(tf_text)\n",
        "    \n",
        "                      \n",
        "    return tf \n",
        "\n",
        "def idf(tf,word):\n",
        "    nd = len(tf)\n",
        "    ndt = 0\n",
        "    idf_ = {}\n",
        "    for text in tf :\n",
        "        if word in text:\n",
        "            ndt += 1\n",
        "    idf = np.log( ((1 + nd) / (1 + ndt)) + 1)\n",
        "    \n",
        "    return idf\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8yyUQ-_urWW5",
        "outputId": "5d6e18f1-4069-4443-ee63-df0cf12e7f9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "\n",
        "def tf_idf(document):\n",
        "    tf_ = tf(document)\n",
        "    idf_ = {}\n",
        "   \n",
        "    \n",
        "    for text in tqdm(document):\n",
        "        for word in text:\n",
        "            idf_[word] = idf(tf_,word)\n",
        "    \n",
        "    tfidf = tf_\n",
        "    for index, text in enumerate(tf_):\n",
        "        for word in text:\n",
        "            tfidf[index][word] = text[word] * idf_[word]\n",
        "    \n",
        "    vector = np.zeros((len(tf_), len(idf_)))\n",
        "    \n",
        "    for i, (k, v) in tqdm(enumerate(idf_.items())):\n",
        "        temp = (k, v)\n",
        "        for index, text in enumerate(tfidf):\n",
        "            if k in text:\n",
        "                vector[index][i] = text[k]\n",
        "            else :\n",
        "                \n",
        "                vector[index][i] = 0\n",
        "    \n",
        "           \n",
        "  # Your code\n",
        "    return vector, idf_\n",
        "tfidfmatrix, idf_train = tf_idf(document)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:00<00:00, 135047.46it/s]\n",
            "100%|██████████| 2000/2000 [00:02<00:00, 921.65it/s]\n",
            "5581it [00:04, 1308.80it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tSDMXv46CWwm"
      },
      "source": [
        "# 4) Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mRpciSZZ-mb9"
      },
      "source": [
        "![alt text](https://cdn.lynda.com/course/578082/578082-637075371482276339-16x9.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tk1EJq7B4JwX"
      },
      "source": [
        "### 4.1) Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MoiyVNNz-q5k"
      },
      "source": [
        "The Logistic Regression classifier must be implemented from scratch here.\n",
        "\n",
        "You can fit the training data into the classifier after implementing linear regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5q-s73zI-xOq",
        "colab": {}
      },
      "source": [
        "Y_train = trainset['cat1'].values.reshape((1, 700000))\n",
        "Y_trainset = Y_train[0][start_point : end_point]\n",
        "X = tfidfmatrix\n",
        "def Y_vectorization(Y_trainset):\n",
        "    vectorized_y = np.zeros((Y_trainset.shape[0], 6))\n",
        "    for index, word in enumerate(Y_trainset) :\n",
        "        if word == 'businesses':\n",
        "            vectorized_y[index][0] = 1\n",
        "        elif word == 'electronic-devices':\n",
        "            vectorized_y[index][1] = 1\n",
        "        elif word == 'for-the-home':\n",
        "            vectorized_y[index][2] = 1\n",
        "        elif word == 'leisure-hobbies':\n",
        "            vectorized_y[index][3] = 1\n",
        "        elif word == 'personal':\n",
        "            vectorized_y[index][4] = 1\n",
        "        elif word == 'vehicles':\n",
        "            vectorized_y[index][5] = 1\n",
        "    return vectorized_y\n",
        "m = tfidfmatrix.shape[0]\n",
        "number_of_features = tfidfmatrix.shape[1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ehDwZnwr3RVc",
        "colab_type": "code",
        "outputId": "35c0747c-6f38-404e-94c2-73fb9e431202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X = tfidfmatrix\n",
        "Y = Y_vectorization(Y_trainset)\n",
        "m = X.shape[0]\n",
        "number_of_features = X.shape[1]\n",
        "learning_rate = 0.05\n",
        "\n",
        "\n",
        "\n",
        "w = np.zeros((number_of_features, 6))\n",
        "Z = np.dot(X, w).reshape((m, 6)) \n",
        "\n",
        "def softmax(Z):\n",
        "    Z = np.array(Z)\n",
        "    output = np.zeros((Z.shape[0], Z.shape[1]))\n",
        "    for index, row in enumerate(Z) :\n",
        "        for index1, column in enumerate(row):\n",
        "            output[index][index1] = np.exp(column) / np.sum(np.exp(row))\n",
        "    return output\n",
        "\n",
        "def gradient_descent(X, Z, Y):\n",
        "    return (np.dot(X.T, (Z[0] - Y)) / len(Y)).reshape((number_of_features, 6))\n",
        "\n",
        "def update_weight_loss(w, learning_rate, gradient):\n",
        "    return w - learning_rate * gradient\n",
        "\n",
        "for i in tqdm(range(0, 2000)) :\n",
        "    Z = np.dot(X, w).reshape((m, 6))\n",
        "    A = softmax(Z)\n",
        "    gradient = gradient_descent(X, A, Y)\n",
        "    w = update_weight_loss(w, learning_rate, gradient)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [07:07<00:00,  4.67it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S310cjZ13RVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def final(A):\n",
        "    output = np.zeros((A.shape[0], A.shape[1]))\n",
        "    for index, row in enumerate(A):\n",
        "        for index1, column in enumerate(row):\n",
        "            if column == max(row):\n",
        "                output[index][index1] = 1\n",
        "    return output\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfokziSw3RVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W_for_unique_words = {}\n",
        "for i, (k, v) in enumerate(idf_train.items()):\n",
        "    W_for_unique_words[k] = w[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k089_muZ6j-j"
      },
      "source": [
        "## 5) Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kc5dl49O3aST"
      },
      "source": [
        "Now you can predict a category for each of the validation data using the implemented classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xnv1hMOm3RVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################### validationset #####################\n",
        "testset_start_point = 100000\n",
        "testset_end_point = 100010\n",
        "# a = validationset.iloc[testset_start_point : testset_end_point, 9].values\n",
        "# print(a)\n",
        "#############################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TkBi_V83RVk",
        "colab_type": "code",
        "outputId": "56e259c4-ce96-4e40-acb5-a95f8609b935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Y_valid = validationset['cat1'].values.reshape((1, valid_len))\n",
        "# Y_testset = Y_valid[0][testset_start_point : testset_end_point]\n",
        "# Y_actual = Y_vectorization(Y_testset)\n",
        "# A = validationset.iloc[testset_start_point : testset_end_point, 9].values\n",
        "# print(A)\n",
        "A = ['پلی استیشن 2درحدصفرو واقعا نونوهستش با دو دستگیره نونو دستگیره هارو تازه   صفر خریدم تو عکس مشخصه نونو هستش همه بازیهارو ب راحتی بالا میاره بهمراه تست خودتون یا علی',\n",
        "'بیمه یکسال موتوری پلمپ ب شرط میدم اچار نخورده دزدگیر از راه دور جک سنسور دار   لاستیک جلو عقب نو لنت نو فقط باتریش خوابیده استفاده نکردم چندوقت باتریش ضعیف شده هیچ مشکل دیگه ی نداره',\n",
        "'میز   و صندلی نهارخوری 4نفره چوبی, به رنگ  قهوه ای تیره با شیشه 10میل; جنس چوب مرغوب, سالم و تمیز',\n",
        "' دور رنگ دوگانه دستی و شتاب اولیه هم تراز با بنزین بیمه بدنه دارد در صورت نخواستن کپسول سی ان جی برداشته میشود و میلغ ٢ میلیون تومان از کسر خواهد شد',\n",
        "'بی رنگ بیمه تا برج 7',\n",
        "'رنگ یشمی، عقب وجلو بدون تصادف، بیمه تااخرسال وشش سال تخفیف بیمه،  چهارحلقه لاستیک نو، باتری 88امپرنوباگارنتیش، رادیات دولول نو.موتورتازه.تعمیربه.شرط',\n",
        "'شش جفت   قناری سفیدطلایی باقفس وکاورآوازخوان به فروش می رسدجفتی80000هزارتومان',\n",
        "'گوشی هواوی وای 560 تمیز با جعبه و شارژر',\n",
        "'محصول تیم خیریه ی خانه خلاقیت و هنر.ساخته شده از چوب نرّاد خشک روسی.گارانتی مادام العمر.ارسال رایگان.استحکام و ظرافت مثال زدنی.',\n",
        "'باسلام..به سیستم در حد نو فقط یه دو ماه رو ماشین نصب بوده...با نو هیچ فرقی نمیکنه،،به شرط وتضمین،ضبط پایونر x1درحد نو مخصوص سیستم خیلی امکانات داره 4کاناله ',\n",
        "'فقط روکش مبل آسیب دیده سه تا پایه مبل دیگه هیچ مشکلی نداره سالمه میز عسلی جلومبلی کاملا سالم نو .به خریدار واقعی تخفیف هم میدم چون جاندارم میخوام بفروشم',\n",
        "'به دلیل اینکه  میخوام دکوراسیون خونمو عوض کنم میفروشمش به قیمت خیلی مناسب',\n",
        "'رینگ ۱۸.۲۲۵.۶۵ یدونه ضعیف هستش.مشتری واقعی زنگ بزند ممنون.تالش',\n",
        "'یک مکانی امن برای استراحت کردن، زندگی کردن و آسودن است در این مکان معمولاً یک فرد یا یک خانواده زندگی می‌کنند و وسایل خود را در آن نگهداری می‌کنند در فرهنگ سنتی و عامه ایرانی گاه از اصطلاح «چهاردیواری (چاردیواری)» به جای  استفاده می‌شود.[۱‌ جدید شامل سرویس‌های بهداشتی و امکانات تهیه غذا نیز هستند اما در برخی مناطق همچنان برخی اقوام دارای خانه‌هایی هستند که پیشرفت نکرده، از امکانات رفاهی یا حتی مکان ثابتی برخوردار نیست. چادرنشینی یکی از انواع این نوع زندگی است.']\n",
        "B = np.array(A)\n",
        "print(len(B))\n",
        "document_test = preprocessing(A)\n",
        "# print(document_test)\n",
        "\n",
        "tf_test = tf(document_test)\n",
        "vector_test, idf_test = tf_idf(document_test)\n",
        "w_test = np.zeros((len(idf_test), 6))\n",
        "for index, (word, value) in enumerate(idf_test.items()):             \n",
        "     if word in W_for_unique_words:\n",
        "\n",
        "        w_test[index]= W_for_unique_words[word]\n",
        "     else:\n",
        "         pass\n",
        "\n",
        "Z_test = np.dot(vector_test, w_test)\n",
        "A_test = softmax(Z_test)\n",
        "Y_prediction = final(A_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:00<00:00, 259.90it/s]\n",
            "100%|██████████| 14/14 [00:00<00:00, 7893.57it/s]\n",
            "100%|██████████| 14/14 [00:00<00:00, 30159.35it/s]\n",
            "100%|██████████| 14/14 [00:00<00:00, 8415.06it/s]\n",
            "195it [00:00, 86003.08it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDOknNR8qZs3",
        "colab_type": "code",
        "outputId": "39670e53-1c56-4b2d-c791-27ada3decbf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "print(Y_prediction)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhqKeIeX3RVm",
        "colab_type": "code",
        "outputId": "63217cfe-e603-4929-f3a2-160e1a3cbcde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "counter = 0\n",
        "for index, row in enumerate(Y_actual):\n",
        "    for index1, column in enumerate(row) :\n",
        "        if column == 1:\n",
        "            if Y_prediction[index][index1] == 1:\n",
        "                counter += 1\n",
        "            else :\n",
        "                pass\n",
        "        else :\n",
        "            pass\n",
        "print(counter)\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JsxqAwVvwOdD"
      },
      "source": [
        "## 6) Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "75Ar7r9pyRLq"
      },
      "source": [
        "It's time to evaluate your model using predicted categories for validation data.\n",
        "\n",
        "You need to create a confusion matrix based on your prediction and the real labels. Then you can use this confusion matrix for calculation other measuring metrics. \n",
        "\n",
        "As this problem is a multi-class problem, the calculation formula is a little different from the general case. Read [this article](https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WZ-19cA3wiqT"
      },
      "source": [
        "### 6.1) Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KB5CCVKbw2Po",
        "outputId": "a6afbe7e-9843-4c4a-f453-104c5de7789c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Your implementation\n",
        "\n",
        "confusion_matrix = np.zeros((6, 6))\n",
        "\n",
        "for row_index_prediction, row in enumerate(Y_prediction):\n",
        "    for column_index_prediction, column in enumerate(row):\n",
        "        if column == 1:\n",
        "            if Y_actual[row_index_prediction][column_index_prediction] == 1:\n",
        "                confusion_matrix[column_index_prediction][column_index_prediction] += 1\n",
        "            else:\n",
        "                for column_index_actual, i in enumerate(Y_actual[row_index_prediction]):\n",
        "                    if i == 1:\n",
        "                        confusion_matrix[column_index_prediction][column_index_actual] +=1\n",
        "                    else :\n",
        "                        pass\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "print(confusion_matrix)\n",
        "                    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  8.   9.  13.   6.   4.   6.]\n",
            " [ 17. 242.  54.  20.  27.  27.]\n",
            " [ 71.  77. 522.  57.  99.  48.]\n",
            " [  3.   8.  20.  75.   8.  12.]\n",
            " [  9.   9.  18.  11. 124.   7.]\n",
            " [ 23.  52.  69.  32.  36. 337.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SM6GgtHNwl1a"
      },
      "source": [
        "### 6.2) Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9hL4wy510lEw"
      },
      "source": [
        "$$\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eXPscPgkw4_S",
        "outputId": "c7ab5a8b-6d5a-409a-9107-0d33f599d4c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# Your code\n",
        "axis0 = np.sum(confusion_matrix, axis = 0)\n",
        "accuracy = np.zeros((confusion_matrix.shape[0], 1))\n",
        "for i in range(confusion_matrix.shape[0]):\n",
        "  TP = confusion_matrix[i][i]\n",
        "  FP = np.sum(confusion_matrix[i]) - confusion_matrix[i][i]\n",
        "  FN = axis0[i] - confusion_matrix[i][i]\n",
        "  TN = np.sum(confusion_matrix) - TP -FP -FN\n",
        "  accuracy[i][0] = (TP + TN) / (TP + FN + FP + TN)\n",
        "  print(\"accuracy for class \", cat1[i], \" is : \", accuracy[i][0])\n",
        "mean_accuracy = np.sum(accuracy/accuracy.shape[0])\n",
        "print(mean_accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy for class  businesses  is :  0.9254629629629629\n",
            "accuracy for class  electronic-devices  is :  0.8611111111111112\n",
            "accuracy for class  for-the-home  is :  0.7564814814814815\n",
            "accuracy for class  leisure-hobbies  is :  0.9180555555555555\n",
            "accuracy for class  personal  is :  0.8944444444444445\n",
            "accuracy for class  vehicles  is :  0.8555555555555555\n",
            "0.8685185185185185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nTFur0U5wsmO"
      },
      "source": [
        "### 6.3) Precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "frrTkQWG09fd"
      },
      "source": [
        "$$\\text{Precision} = \\frac{TP}{TP + FP}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q1SGaY9qw7Z4",
        "outputId": "8e59580d-c420-436e-91fd-44ca1a3e3957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Your code\n",
        "\n",
        "cat1 = [\"businesses\", \"electronic-devices\", \"for-the-home\", \"leisure-hobbies\", \"personal\", \"vehicles\"]\n",
        "\n",
        "precision = np.zeros((6, 1))\n",
        "for i in range (0, 6):\n",
        "    precision[i][0] = confusion_matrix[i][i] / np.sum(confusion_matrix[i])\n",
        "    print(\"precision for class \", cat1[i], \" is : \", precision[i][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision for class  businesses  is :  0.17391304347826086\n",
            "precision for class  electronic-devices  is :  0.6253229974160207\n",
            "precision for class  for-the-home  is :  0.597254004576659\n",
            "precision for class  leisure-hobbies  is :  0.5952380952380952\n",
            "precision for class  personal  is :  0.6966292134831461\n",
            "precision for class  vehicles  is :  0.6138433515482696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lQcpDrm1wuhU"
      },
      "source": [
        "### 6.4) Recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QBb1eop61JlG"
      },
      "source": [
        "$$\\text{Recall} = \\frac{TP}{TP + FN}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w8bLKc6Ew_iR",
        "outputId": "ed836b09-64d9-4ce7-fb35-4f2ba1bb6096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Your code\n",
        "recall = np.zeros((6, 1))\n",
        "axis0 = np.sum(confusion_matrix, axis = 0)\n",
        "for i in range (0, 6):\n",
        "    recall[i][0] = confusion_matrix[i][i] / axis0[i]\n",
        "    print(\"recall for class \", cat1[i], \" is : \", recall[i][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "recall for class  businesses  is :  0.061068702290076333\n",
            "recall for class  electronic-devices  is :  0.5941644562334217\n",
            "recall for class  for-the-home  is :  0.8826219512195121\n",
            "recall for class  leisure-hobbies  is :  0.3826530612244898\n",
            "recall for class  personal  is :  0.4539249146757679\n",
            "recall for class  vehicles  is :  0.7798594847775175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VWx42Dr5wx4m"
      },
      "source": [
        "### 6.5) F1 score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hdVnZfm21SCM"
      },
      "source": [
        "$$\\text{F1 score} = 2\\times \\frac{(Recall \\times  Precision)}{Recall + Precision}$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4FkCog8ExBXA",
        "outputId": "ddc9c89c-c4a2-48f0-f7f0-761e6cd9f1a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Your code\n",
        "f1_score = np.zeros((6, 1))\n",
        "for i in range(0, 6):\n",
        "    f1_score[i][0] = 2 * (recall[i][0] * precision[i][0]) / recall[i][0] + precision[i][0]\n",
        "    print(\"F1 score for class \", cat1[i], \" is : \", f1_score[i][0])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score for class  businesses  is :  1.0\n",
            "F1 score for class  electronic-devices  is :  2.3252595155709344\n",
            "F1 score for class  for-the-home  is :  1.6782608695652173\n",
            "F1 score for class  leisure-hobbies  is :  2.419354838709677\n",
            "F1 score for class  personal  is :  2.375\n",
            "F1 score for class  vehicles  is :  2.1210191082802545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TZP1GHbtB2T7"
      },
      "source": [
        "## 7) K-Fold Cross Validation *(Optional)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ayL96oOoB-aq"
      },
      "source": [
        "Evaluate your model based on the K-Fold Cross Validation approach. This step is optional and has a few extra points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Zxs_90TC-3b",
        "colab": {}
      },
      "source": [
        "# Your implementation"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}